---
title: "CARE: A Concurrency-Aware Enhanced Lightweight Cache Management Framework"
collection: publications
permalink: /publications/HPCA2023
venue: "The 29th IEEE International Symposium on High-Performance Computer Architecture (HPCA 2023)"
date: 2024-02-25
---


**Background**

While data access concurrency is a common feature in multi-core systems, traditional cache management policies focus primarily on data locality, often neglecting the optimization potential that concurrent accesses offer. This oversight limits cache performance improvements in data-intensive applications.

**Design**

To improve cache efficiency in multi-core systems, there is a need for cache management that considers both data locality and concurrent access patterns. By reducing cache misses that incur high penalties in concurrent scenarios, overall performance can be enhanced, addressing the limitations of locality-only cache policies.

**Key Features**

- **Pure Miss Contribution (PMC)**: A metric that quantifies the performance impact of cache misses, especially under concurrent access conditions, allowing CARE to prioritize high-cost misses for replacement.
- **Dynamic Threshold Reconfiguration Mechanism (DTRM)**: CARE’s DTRM dynamically adjusts cache thresholds based on PMC, aligning cache behavior with varying workload demands for enhanced efficiency.

**Results**

CARE achieves substantial improvements over traditional LRU policies, with an average 10.3% IPC gain in 4-core systems and up to 17.1% IPC improvement in 16-core systems. These results demonstrate CARE’s scalability and effectiveness in high-concurrency environments.

**Conclusion**

By integrating concurrency-aware metrics and adaptive mechanisms, CARE provides a robust approach to improving cache efficiency in multi-core systems, addressing both locality and concurrency to mitigate the Memory Wall problem.




[paper](../files/HPCA2023/CARE_paper.pdf) [slides](../files/HPCA2023/CARE_slides.pdf) [code](https://github.com/Xiaoyang-Lu/HPCA_23)
